<!DOCTYPE html>
<!--
                      __  .__              ________ 
  ______ ____   _____/  |_|__| ____   ____/   __   \
 /  ___// __ \_/ ___\   __\  |/  _ \ /    \____    /
 \___ \\  ___/\  \___|  | |  (  <_> )   |  \ /    / 
/____  >\___  >\___  >__| |__|\____/|___|  //____/ .co.uk 
     \/     \/     \/                    \/        
-->

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>3D Organ Reconstruction</title>
  <meta name="author" content="Benjamin Blundell" />
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Section9 ATOM Feed">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


  <link href="/css/bootstrap.css" rel="stylesheet">
  <link href="/css/bootstrap-responsive.css" rel="stylesheet">
  <link href="/css/s9.css" rel="stylesheet" media="screen">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/s9-144.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/s9-114.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/s9-72.png">
  <link rel="apple-touch-icon-precomposed" href="/images/s9-57.png">
  <link rel="shortcut icon" href="/images/favicon.ico">

  <!-- META DATA - Need to Check and Update -->
  <meta name="google-site-verification" content="Ew7bGq_-B5Djz3nNvxYt6abpW2fUBOU-rFKrlvIQwiU" /> 

  <!-- General metadata -->
  <meta name="generator" content="" />
  <meta name="robots" content="index,follow" />
  <meta name="revisit-after" content="2 days" />
  <meta name="Author-Template" content="" />
  <meta name="Author" content="Benjamin Blundell" />
  <meta name="Publisher" content="Benjamin Blundell" />
  <meta name="Publisher-Email" content="oni@section9.co.uk" />
  <meta name="Coverage" content="UK" /> 

  <!-- Dublin Core Metadata -->
  <meta name="DC.creator" lang="en" content="Benjamin Blundell" />
  <meta name="DC.date.created" lang="en" content="" />
  <meta name="DC.format" lang="en" content="text/html" />
  <meta name="DC.language" content="en" />
  <meta name="DC.publisher" lang="en" content="Benjamin Blundell" />
  <meta name="DC.rights.copyright" lang="en" content="Benjamin Blundell" />
  <meta name="DC.coverage" lang="en" content="" />
  <meta name="DC.identifier" content="" />    
  
  <!-- eGMS Metadata -->
  <meta name="eGMS.status" lang="en" content="V1.0 Public Consumption" />
  
  <meta name="Keywords" content="creative technologist,webgl, coffeescript, git, kinect,opengl,cinder,openframeworks,cpp,c++,programming,graphics,art,design,code" />
  <meta name="Description" content="Benjamin Blundell, section9 dot co dot uk ltd, the home of Benjamin Blundell. I create with WebGL, Nodejs, Javascript, C++, OpenFrameworks and Cinder amongst many other things." />
  <meta name="DC.title" lang="en" content="section9 dot co dot uk ltd - Home of Creative Technologist Benjamin Blundell" />
  <meta name="DC.description" lang="en" content="" />
  <meta name="DC.subject" lang="en" content="" /> 

  <script src="/js/jquery-1.8.3.min.js"></script>
  <script src="/js/bootstrap.min.js"></script>

</head>

<body>
<div class="navbar navbar-top">
  <div class="navbar-inner">
    <div class="container-fluid">
      <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="brand" href="http://www.section9.co.uk"><img class="brand" src = "/images/s9black.png" alt="section9 dot co dot uk ltd" /></a>
      <a class="brand" href="http://www.section9.co.uk" id="companyname">section9 dot co dot uk</a>

      <div class="nav-collapse collapse">
        <ul class="nav">
          <li><a href="/index.html">Home</a></li>
          <li><a href="/about.html">About</a></li>
          <li><a href="/contact.html">Contact</a></li>
          <li><a href="/blog.html">Blog</a></li>
        </ul>
      </div><!--/.nav-collapse -->
    </div>
  </div>
</div>

<hr />

<div class="container-fluid" id="content">
  <div id="blogblocks">
    
<p>For the last 5 months I've been working with The University of Leeds on a rather interesting project that deals with Keyhole surgery. We are trying to recreate a 3D view from a series of cameras that are fixed inside the patient. To test these ideas we need to build a mockup of the system and work with models of the organs we wish to scan. The result looks something like this.</p>
<iframe src="http://player.vimeo.com/video/47714478" width="460" height="320" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>

<p>I had a lot of false starts and worries about this project, not having done a lot of computer vision before. Its a tricky subject I'd like to know more about and I think some back-to-school maths courses might be needed. Initially, the plan was to work with a stereo setup, as described in the <a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.htmlstereorectify">OpenCV literature</a>. This is known as disparity mapping and it relies somewhat on being able to match one point in one image to the other. One can then work out the <a href="http://en.wikipedia.org/wiki/Epipolar_geometry">Epipolar Geometry</a>  and recompute the depth, creating a 2D map of disparity or depth.</p>
<p>The first step in any camera based solution is <a href="http://en.wikipedia.org/wiki/Camera_resectioning">Camera Resectioning</a> or Camera Calibration. All cameras, especially cheap ones, have lens distortion that needs to be accounted for in each setting. These are known as the <a href="http://en.wikipedia.org/wiki/Intrinsic_parametersIntrinsic_parameters">Intrinsic Parameters</a> .</p>
<p>You can see these arent <em>too</em> hard to understand. The issue here is that we need some object in the world that is regular. Enter the chessboard!  Yes, if you were ever wondering why chessboards are involved, now you know why. The chessboard is regular and fixed. Its planar and the points within the intersections should be regular. If they aren't,  you know you have distortion to fix. You can see how to do this in the <a href="http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.htmlcameracalibrationsquarechessboardtutorial">Lovely OpenCV tutorial</a>.</p>
<p>The C910 does have a little lens distortion but that can be calibrated once and then saved. We then need to know the extrinsic values. This is how the camera relates to another point in the real world. In the case of stereo mapping, we are translating from one camera to another. There is an OpenCV function called <a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.htmlstereorectify">stereoRectify</a>. This is fine for disparity mapping but, as seen in the video, this really proved difficult to work with; too much noise and unreliable results. The main reason is that stereo mapping is tricky. Imagine trying to map one set of pixels to another inside the human body where everything is all a little messy and homogenous. Its probably not going to work.</p>
<p>So, we decided we would just identify one point in all 8 cameras. For that to work, you need to reorganise the Camera Extrinsic equation.</p>
<p>Basically, we have a translation, a rotation and a set of intrinsic parameters that turn a 3D point into a 2D one.  Now, we have 8 2D points that <em>should</em> map onto the same depth. Of course, the above equation, when reorganised, gives the equation for a line. Remember the classic Father Ted video?</p>
<p><img alt="Point Cloud" src="http://farm9.staticflickr.com/8024/7407179758_226e1e17f9.jpg" /></p>
<p><img alt="Greedy Projection" src="http://farm8.staticflickr.com/7269/7407179846_e4fc587e35.jpg" /></p>
<iframe width="460" height="280" src="http://www.youtube.com/embed/25N-4zrk390?feature=player_detailpaget=32s" frameborder="0" allowfullscreen></iframe>

<p>Basically, with just one frame of reference you can have a line on which this point exists in 3D space. We need at least two lines - a triangulation if you will - before we can safely say where in 3D space this point lies. There is a further problem. Due to errors and noise, these lines may not intersect, so no exact solution exists. We need to minimise these line equations. Apparently, there is a well known method (that I didn't know of at the time) called <a href="http://en.wikipedia.org/wiki/Linear_programming">Linear Programming</a> . We have, in effect a system of <a href="http://en.wikipedia.org/wiki/Systems_of_linear_equations">Linear Equations</a> that are <a href="http://en.wikipedia.org/wiki/Overdetermined_system">Overdetermined</a>.  Since they are overdetermined we can use a solver such as <a href="http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.htmlsolve">cvSolve</a> or <a href="http://www.netlib.org/lapack/">LAPACK</a>.</p>
<p>We need to organise our equation above into a set of linear equations in terms of, U,V and 1.</p>
<p><img alt="Final Result" src="http://farm8.staticflickr.com/7118/7699005862_6c75aca6a4.jpg" /></p>
<p>Initially, this setup wasn't too bad. The projector would shine a point which would be picked up by the cameras. This point is then sent to our solver and bam, we have a point in the X,Y,Z space. It does take rather a long time though and the main problem is the projector can only <em>see</em> the top of the model. We need to use a laser pointer to see the sides of the model.</p>
<p>So how do we go from a point cloud to an actual mesh? This bit is slightly trickier. Fortunately, we have a set of libraries - <a href="http://pointclouds.org/">The Point Cloud Library</a> in fact.</p>
<p>There are a few tutorials out there for reconstruction with the PCL  library. I tried the Greedy Projection algorithm but the results didn't quite work out. I moved over to using the Poission algorithm. This exists in MeshLab also. It requires quite a complete point cloud and a set of normals. One can estimate the normals of a point cloud by uisng the <a href="http://www.pointclouds.org/documentation/tutorials/resampling.php">Moving Least Squares</a> algorithm. The results look a little like this:We can also do a little statistical analysis to remove noise and create a smoother mesh.</p>
<p><img alt="MLS" src="http://farm9.staticflickr.com/8012/7449154248_e40c236777.jpg" /></p>
<p><img alt="meshlab mesh" src="http://farm9.staticflickr.com/8017/7630835790_30dea1be0c.jpg" /></p>
<p>The final step is to colour the mesh. Now here we have a problem. How do you choose the colour at a point? The trick is the try and match the normals. Since we know the extrinsic values of the cameras, we can work out the plane of the camera and the normal to the plane. We know the normal of the triangle so the closer the dot product is to 1, the more of that triangle can be seen from that camera. This is slightly naive but works ok. There are other considerations such as the distance between the triangle and the camera. This is something we can't work out easily unfortunately (though I'm sure there is a way). An enhancement to this algorithm is to try and reduce the patchwork look by comparing neighbouring triangles. This requires the generation of a <a href="http://en.wikipedia.org/wiki/Winged_edge">Winged Edge</a> structure that is quite time consuming.</p>
<p><img alt="box setup" src="http://farm6.staticflickr.com/5235/7087159303_ef0ebd5a4f.jpg" /></p>
<p>My solution to this problem is to upload the camera normals and all 8 textures to the pixel shader. We then choose the texture by comparing the normals in shader space. Although we only need to do this once, by doing this in parallel we get a much faster result. I ignore the neighbouring triangle issue as the look isn't much better.</p>
<p>There are quite a few improvements and other ideas I have in my head that I might get chance to try in the future. So far though, I'm quite pleased with the result. I'd like to thank Derek Magee, Peter Culmer, Katie Eagleton, Chris Paton, Roxlu and London Hackspace for all the help.</p>

  </div>

  <hr />
  <footer>
    <p><ul><li>© section9 dot co dot uk ltd 2013 - Benjamin Blundell - Registered Company 7794714 - Unit 10 Durham Yard, Teesdale Street, London E2 6QF</li><li> <a href="http://www.section9.co.uk/atom.xml">rss atom feed</a></li><li><a href="http://www.section9.co.uk/key.html">public gnupg key</a></li></p>
  </footer>
</div>



<script type="text/javascript">
  $(window).load(function(){

    // Process p code blocks

    $("#blogblocks p > code").each(function (i,e) {
      $(e).parent().replaceWith(e);
      $(e).wrap("<div class='blogvideo'><div>")
    });


    // Process 'objects' next

    $("#blogblocks p > object").each(function (i,e) {
      $(e).parent().replaceWith(e);
    });
    

    $('#blogblocks p').wrap("<div class='span3 blogblock'></div>");

    var divs = $("#blogblocks .blogblock");
    for(var i = 0; i < divs.length; i+=4) {
      divs.slice(i, i+4).wrapAll("<div class='row-fluid'></div>");
    }

    $('#blogblocks .row-fluid').each (function (i,e) {
     $(e).after('<hr />');
    });

    $("#blogblocks iframe").wrap("<div class='blogvideo'><div>")
    $("#blogblocks object").wrap("<div class='blogvideo'><div>")


    $("#blogblocks .blogvideo").each(function(i,e){
      $(e).after('<hr />');

    });

    // Process all images in the post to add light boxes
    var images = $("#blogblocks img");
    for(var i = 0; i < images.length; i++) {
      $(images[i]).wrap("<a href='" + $(images[i]).attr("src") + "'></a>");
    }

  });

</script>


</body>
</html>

