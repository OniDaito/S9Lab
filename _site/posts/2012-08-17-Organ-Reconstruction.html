
<!DOCTYPE html>
<!--
                      __  .__              ________ 
  ______ ____   _____/  |_|__| ____   ____/   __   \
 /  ___// __ \_/ ___\   __\  |/  _ \ /    \____    /
 \___ \\  ___/\  \___|  | |  (  <_> )   |  \ /    / 
/____  >\___  >\___  >__| |__|\____/|___|  //____/ .co.uk 
     \/     \/     \/                    \/        
-->

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Section9 3D Organ Reconstruction</title>
  <meta name="author" content="Benjamin Blundell" />
  <link href="http://feeds.feedburner.com/secti0n9" rel="alternate" title="Section9 Lab - Benjamin Blundell" type="application/atom+xml" />
  <link href="/css/bootstrap.min.css" rel="stylesheet" media="screen">
  <link href="/css/s9.css" rel="stylesheet" media="screen">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
   <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/s9-144.png">
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/s9-114.png">
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/s9-72.png">
  <link rel="apple-touch-icon-precomposed" href="/images/s9-57.png">
  <link rel="shortcut icon" href="/favicon.ico">
  
  <!-- META DATA - Need to Check and Update -->
  <meta name="google-site-verification" content="Ew7bGq_-B5Djz3nNvxYt6abpW2fUBOU-rFKrlvIQwiU" /> 

  <!-- General metadata -->
  <meta name="generator" content="" />
  <meta name="robots" content="index,follow" />
  <meta name="revisit-after" content="2 days" />
  <meta name="Author-Template" content="" />
  <meta name="Author" content="Benjamin Blundell" />
  <meta name="Publisher" content="Benjamin Blundell" />
  <meta name="Publisher-Email" content="oni@section9.co.uk" />
  <meta name="Coverage" content="UK" /> 

  <!-- Dublin Core Metadata -->
  <meta name="DC.creator" lang="en" content="Benjamin Blundell" />
  <meta name="DC.date.created" lang="en" content="" />
  <meta name="DC.format" lang="en" content="text/html" />
  <meta name="DC.language" content="en" />
  <meta name="DC.publisher" lang="en" content="Benjamin Blundell" />
  <meta name="DC.rights.copyright" lang="en" content="Benjamin Blundell" />
  <meta name="DC.coverage" lang="en" content="" />
  <meta name="DC.identifier" content="" />    
  
  <!-- eGMS Metadata -->
  <meta name="eGMS.status" lang="en" content="V1.0 Public Consumption" />
  
  <meta name="Keywords" content="creative technologist,kinect,opengl,cinder,openframeworks,cpp,c++,programming,graphics,art,design,code" />
  <meta name="Description" content="Benjamin Blundell, section9 dot co dot uk ltd, the home of Creative Technologist Benjamin Blundell. I play with WebGL, Nodejs, Javascript, C++, OpenFrameworks and Cinder amongst many other things." />
  <meta name="DC.title" lang="en" content="section9 dot co dot uk ltd - Home of Creative Technologist Benjamin Blundell" />
  <meta name="DC.description" lang="en" content="" />
  <meta name="DC.subject" lang="en" content="" /> 

  <script src="/js/jquery-1.8.3.min.js"></script>

  <style type="text/css">
    body {
      
        background: url(/images/strap/leeds.jpg) no-repeat center center fixed; 
      
      -webkit-background-size: cover;
      -moz-background-size: cover;
      -o-background-size: cover;
      background-size: cover;
    }
  </style>

</head>

<body>
<div id="wrap">
  <div class="container-fluid clear-top" id="main">
    <div class="navbar navbar-inverse">
      <div class="navbar-inner">
        <!-- Responsive Navbar Part 1: Button for triggering responsive navbar (not covered in tutorial). Include responsive CSS to utilize. -->
        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </a>
        <a class="brand" href="#"><img src="/images/s9white_small.png" alt="section9"></a>
        <!-- Responsive Navbar Part 2: Place all navbar contents you want collapsed withing .navbar-collapse.collapse. -->
        <div class="nav-collapse collapse">
          <ul class="nav">
            <li ><a href="/index.html">Home</a></li>
            <li ><a href="/projects.html">Selected Works</a></li>
            <li ><a href="/about.html">About</a></li>
          </ul>
          <div class="social_container">
           <a href="http://www.flickr.com/photos/section9/"><div class="social" id="flickr">&nbsp;</div></a>
            <a href="http://www.twitter.com/secti0n9/"><div class="social" id="twitter">&nbsp;</div></a>
            <a href="http://uk.linkedin.com/in/section9/"><div class="social" id="linkedin">&nbsp;</div></a>
            <a href="https://github.com/OniDaito/"><div class="social" id="github">&nbsp;</div></a>
          </div>
        </div>
      </div>
   </div>

    <div id="content">
      <div class="box">
        <h3>3D Organ Reconstruction</h3>
        <span class="lead">2012-08-17 00:00:00</span>
      </div>
      
<div class="s2 box"><p>For the last 5 months I've been working with The University of Leeds on a rather interesting project that deals with Keyhole surgery. We are trying to recreate a 3D view from a series of cameras that are fixed inside the patient. To test these ideas we need to build a mockup of the system and work with models of the organs we wish to scan. The result looks something like this.</p>
</div><div class="s4 box"><p><iframe src="http://player.vimeo.com/video/47714478" width="460" height="320" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>
</div><div class="s2 box"><p>I had a lot of false starts and worries about this project, not having done a lot of computer vision before. Its a tricky subject I'd like to know more about and I think some back-to-school maths courses might be needed. Initially, the plan was to work with a stereo setup, as described in the <a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#stereorectify">OpenCV literature</a>. This is known as disparity mapping and it relies somewhat on being able to match one point in one image to the other. One can then work out the <a href="http://en.wikipedia.org/wiki/Epipolar_geometry">Epipolar Geometry</a>  and recompute the depth, creating a 2D map of disparity or depth.</p>
<p>The first step in any camera based solution is <a href="http://en.wikipedia.org/wiki/Camera_resectioning">Camera Resectioning</a> or Camera Calibration. All cameras, especially cheap ones, have lens distortion that needs to be accounted for in each setting. These are known as the <a href="http://en.wikipedia.org/wiki/Intrinsic_parameters#Intrinsic_parameters">Intrinsic Parameters</a> .</p>
</div><div class="s2 box"><p>You can see these arent <em>too</em> hard to understand. The issue here is that we need some object in the world that is regular. Enter the chessboard!  Yes, if you were ever wondering why chessboards are involved, now you know why. The chessboard is regular and fixed. Its planar and the points within the intersections should be regular. If they aren't,  you know you have distortion to fix. You can see how to do this in the <a href="http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration_square_chess/camera_calibration_square_chess.html#cameracalibrationsquarechessboardtutorial">Lovely OpenCV tutorial</a>.</p>
<p>The C910 does have a little lens distortion but that can be calibrated once and then saved. We then need to know the extrinsic values. This is how the camera relates to another point in the real world. In the case of stereo mapping, we are translating from one camera to another. There is an OpenCV function called <a href="http://docs.opencv.org/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#stereorectify">stereoRectify</a>. This is fine for disparity mapping but, as seen in the video, this really proved difficult to work with; too much noise and unreliable results. The main reason is that stereo mapping is tricky. Imagine trying to map one set of pixels to another inside the human body where everything is all a little messy and homogenous. Its probably not going to work.</p>
<p>So, we decided we would just identify one point in all 8 cameras. For that to work, you need to reorganise the Camera Extrinsic equation.</p>
<p>Basically, we have a translation, a rotation and a set of intrinsic parameters that turn a 3D point into a 2D one.  Now, we have 8 2D points that <em>should</em> map onto the same depth. Of course, the above equation, when reorganised, gives the equation for a line. Remember the classic Father Ted video?</p>
</div><div class="s3 box"><p><img alt="Point Cloud" src="http://farm9.staticflickr.com/8024/7407179758_226e1e17f9.jpg" /></p>
</div><div class="s3 box"><p><img alt="Greedy Projection" src="http://farm8.staticflickr.com/7269/7407179846_e4fc587e35.jpg" /></p>
</div><div class="s4 box"><p><iframe width="460" height="280" src="http://www.youtube.com/embed/25N-4zrk390?feature=player_detailpage#t=32s" frameborder="0" allowfullscreen></iframe></p>
</div><div class="s2 box"><p>Basically, with just one frame of reference you can have a line on which this point exists in 3D space. We need at least two lines - a triangulation if you will - before we can safely say where in 3D space this point lies. There is a further problem. Due to errors and noise, these lines may not intersect, so no exact solution exists. We need to minimise these line equations. Apparently, there is a well known method (that I didn't know of at the time) called <a href="http://en.wikipedia.org/wiki/Linear_programming">Linear Programming</a> . We have, in effect a system of <a href="http://en.wikipedia.org/wiki/Systems_of_linear_equations">Linear Equations</a> that are <a href="http://en.wikipedia.org/wiki/Overdetermined_system">Overdetermined</a>.  Since they are overdetermined we can use a solver such as <a href="http://opencv.willowgarage.com/documentation/cpp/core_operations_on_arrays.html#solve">cvSolve</a> or <a href="http://www.netlib.org/lapack/">LAPACK</a>.</p>
<p>We need to organise our equation above into a set of linear equations in terms of, U,V and 1.</p>
</div><div class="s3 box"><p><img alt="Final Result" src="http://farm8.staticflickr.com/7118/7699005862_6c75aca6a4.jpg" /></p>
</div><div class="s3 box"><p>Initially, this setup wasn't too bad. The projector would shine a point which would be picked up by the cameras. This point is then sent to our solver and bam, we have a point in the X,Y,Z space. It does take rather a long time though and the main problem is the projector can only <em>see</em> the top of the model. We need to use a laser pointer to see the sides of the model.</p>
<p>So how do we go from a point cloud to an actual mesh? This bit is slightly trickier. Fortunately, we have a set of libraries - <a href="http://pointclouds.org/">The Point Cloud Library</a> in fact.</p>
</div><div class="s2 box"><p>There are a few tutorials out there for reconstruction with the PCL  library. I tried the Greedy Projection algorithm but the results didn't quite work out. I moved over to using the Poission algorithm. This exists in MeshLab also. It requires quite a complete point cloud and a set of normals. One can estimate the normals of a point cloud by uisng the <a href="http://www.pointclouds.org/documentation/tutorials/resampling.php">Moving Least Squares</a> algorithm. The results look a little like this:We can also do a little statistical analysis to remove noise and create a smoother mesh.</p>
</div><div class="s3 box"><p><img alt="MLS" src="http://farm9.staticflickr.com/8012/7449154248_e40c236777.jpg" /></p>
</div><div class="s3 box"><p><img alt="meshlab mesh" src="http://farm9.staticflickr.com/8017/7630835790_30dea1be0c.jpg" /></p>
</div><div class="s3 box"><p>The final step is to colour the mesh. Now here we have a problem. How do you choose the colour at a point? The trick is the try and match the normals. Since we know the extrinsic values of the cameras, we can work out the plane of the camera and the normal to the plane. We know the normal of the triangle so the closer the dot product is to 1, the more of that triangle can be seen from that camera. This is slightly naive but works ok. There are other considerations such as the distance between the triangle and the camera. This is something we can't work out easily unfortunately (though I'm sure there is a way). An enhancement to this algorithm is to try and reduce the patchwork look by comparing neighbouring triangles. This requires the generation of a <a href="http://en.wikipedia.org/wiki/Winged_edge">Winged Edge</a> structure that is quite time consuming.</p>
</div><div class="s3 box"><p><img alt="box setup" src="http://farm6.staticflickr.com/5235/7087159303_ef0ebd5a4f.jpg" /></p>
</div><div class="s2 box"><p>My solution to this problem is to upload the camera normals and all 8 textures to the pixel shader. We then choose the texture by comparing the normals in shader space. Although we only need to do this once, by doing this in parallel we get a much faster result. I ignore the neighbouring triangle issue as the look isn't much better.</p>
<p>There are quite a few improvements and other ideas I have in my head that I might get chance to try in the future. So far though, I'm quite pleased with the result. I'd like to thank Derek Magee, Peter Culmer, Katie Eagleton, Chris Paton, Roxlu and London Hackspace for all the help.</p>
</div>
      <div class="box">
  <a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="secti0n9">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
      </div>
    </div>
  </div>
</div>

 
  <script src="/js/bootstrap.min.js"></script>
  <script src="/js/jquery.masonry.min.js"></script>
  <script src="/js/jquery.infinitescroll.min.js"></script>


   <script type="text/javascript">
      $(window).load(function(){

        $(function(){
    
          var $container = $('#content');
          
          $container.imagesLoaded(function(){
            $container.masonry({
              itemSelector: '.box',
              columnWidth: 60,
              isAnimated: true,
              cornerStampSelector: '.corner-stamp'
            });
          });
        });
      });


    // Setup the funky links

    var supports3DTransforms =  document.body.style['webkitPerspective'] !== undefined || 
               document.body.style['MozPerspective'] !== undefined;

    function linkify( selector ) {
        if( supports3DTransforms ) {
            
            var nodes = document.querySelectorAll( selector );

            for( var i = 0, len = nodes.length; i < len; i++ ) {
                var node = nodes[i];

                if( !node.className || !node.className.match( /roll/g ) ) {
                    node.className += ' roll';
                    node.innerHTML = '<span data-title="'+ node.text +'">' + node.innerHTML + '</span>';
                }
            };
        }
    }

    linkify( 'p a' );

    </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2107281-3");
pageTracker._trackPageview();
</script>
</body>
</html>